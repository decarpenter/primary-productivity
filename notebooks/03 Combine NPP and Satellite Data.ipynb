{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cda41a-f46a-49d2-a4a4-d9135aa67520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydeck as pdk\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from geopy.distance import geodesic\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from astral.sun import sun\n",
    "from astral.location import Observer\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.spatial import cKDTree\n",
    "import pytz\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "from pyEDM import *\n",
    "import json\n",
    "from pyproj import Proj\n",
    "from pyhdf.SD import SD, SDC\n",
    "from scipy.spatial import cKDTree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c2db8-bd27-41c2-8eb1-e42c163ec3c5",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fad17f1-ee61-4c6d-9f43-9214ecc4696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Integrated NPP Data from CalCOFI Bottle Survey\n",
    "npp = pd.read_csv('../data/CalCOFI_Integrated_NPP.csv', index_col=0)\n",
    "npp = npp[['Date', 'Latitude', 'Longitude', 'Integrated_NPP']]\n",
    "npp['Date'] = pd.to_datetime(npp['Date'])\n",
    "npp = npp.sort_values(by='Date').reset_index(drop=True)\n",
    "npp = npp[npp.Date >= '1997-09-04'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ef42cc-ad4e-40b0-968e-e78f66c49506",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc486118-b431-4e50-bd67-742a256ac3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grid_window(res_km, target_area_km2=729, valid_fraction=1/3):\n",
    "    \"\"\"\n",
    "    Calculate the size of an n x n satellite pixel grid that best approximates a target spatial area, \n",
    "    and determine the minimum number of valid pixels required within that grid based on a valid fraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the ideal grid size (in pixels) to match the target spatial area\n",
    "    ideal_grid_size = math.sqrt(target_area_km2 / (res_km ** 2))\n",
    "    \n",
    "    # Round ideal size to nearest integer to get base grid size\n",
    "    base_grid_size = round(ideal_grid_size)\n",
    "    \n",
    "    # Find nearest odd integer grid sizes: one just below or equal, and one just above\n",
    "    odd_floor = base_grid_size if base_grid_size % 2 == 1 else base_grid_size - 1\n",
    "    odd_ceil = odd_floor + 2  # Next odd number above odd_floor\n",
    "    \n",
    "    # Compute the actual spatial areas these two grid sizes represent\n",
    "    area_floor = (odd_floor * res_km) ** 2\n",
    "    area_ceil = (odd_ceil * res_km) ** 2\n",
    "    \n",
    "    # Choose the odd grid size with the area closest to the target area\n",
    "    if abs(area_floor - target_area_km2) <= abs(area_ceil - target_area_km2):\n",
    "        n = odd_floor\n",
    "    else:\n",
    "        n = odd_ceil\n",
    "    \n",
    "    # Calculate total number of pixels in the grid window\n",
    "    total_pixels = n ** 2\n",
    "    \n",
    "    # Calculate the minimum number of valid pixels required, rounding to nearest integer\n",
    "    min_valid_pixels = int(valid_fraction * total_pixels + 0.5)\n",
    "    \n",
    "    return n, min_valid_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0070f935-f9a0-4452-93b4-b9f2db6f00cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chl_day(filename):\n",
    "    \"\"\"Loads HDF, applies scaling, and returns data + projection info.\"\"\"\n",
    "    hdf = SD(f'../data/chl_data/{filename}', SDC.READ)\n",
    "    \n",
    "    # Identify the primary dataset (usually the first one)\n",
    "    sds_name = list(hdf.datasets().keys())[0]\n",
    "    sds = hdf.select(sds_name)\n",
    "    data = sds[:].astype(np.float32)\n",
    "    \n",
    "    # Get scaling attributes\n",
    "    attrs = sds.attributes()\n",
    "    slope = attrs.get('Slope', 1.0)\n",
    "    intercept = attrs.get('Intercept', 0.0)\n",
    "    base = attrs.get('Base', 10.0)\n",
    "    \n",
    "    # Apply transformation: base ** (slope * data + intercept)\n",
    "    # Masking out land/fill values (usually -1 or 255) to avoid math errors\n",
    "    invalid = (data <= 0) | (data == 255)\n",
    "    chl = np.full(data.shape, np.nan)\n",
    "    chl[~invalid] = base ** (slope * data[~invalid] + intercept)\n",
    "    \n",
    "    return chl #, proj_info\n",
    "\n",
    "# Get latitude/longitude grid\n",
    "hdf_file = \"../data/cal_aco_3840_Latitude_Longitude.hdf\"\n",
    "hdf = SD(hdf_file, SDC.READ)\n",
    "lat_name = list(hdf.datasets().keys())[0]\n",
    "lon_name = list(hdf.datasets().keys())[1]\n",
    "lats = hdf.select(lat_name)\n",
    "lons = hdf.select(lon_name)\n",
    "lats = lats[:]\n",
    "lons = lons[:]\n",
    "\n",
    "# Load chlorophyll file lookup index\n",
    "with open('../data/chl_index.json', 'r') as f:\n",
    "    chl_index = json.load(f)\n",
    "    \n",
    "date = '2006-07-30' # Using your example file date\n",
    "\n",
    "if date in chl_index:\n",
    "    print(chl_index[date])\n",
    "    chl_data = load_chl_day(chl_index[date])\n",
    "\n",
    "    assert chl_data.shape == lats.shape == lons.shape\n",
    "\n",
    "    # Create xarray\n",
    "    time = np.datetime64(date)\n",
    "    chl = chl_data[np.newaxis, :, :]\n",
    "    da = xr.DataArray(\n",
    "        data=chl,\n",
    "        dims=(\"time\", \"y\", \"x\"),\n",
    "        coords={\n",
    "            \"time\": [time],\n",
    "            \"latitude\": ((\"y\", \"x\"), lats),\n",
    "            \"longitude\": ((\"y\", \"x\"), lons),\n",
    "        },\n",
    "        name=\"CHL\",\n",
    "    )\n",
    "\n",
    "########## Visualize CHL data for selected day\n",
    "\n",
    "fig = plt.figure()\n",
    "stride = 5  # try 8–15 if needed\n",
    "\n",
    "da_sub = da.isel(y=slice(None, None, stride),x=slice(None, None, stride),)\n",
    "df = (da_sub.isel(time=0).to_dataframe().reset_index().dropna(subset=[\"CHL\"]))\n",
    "chl = df[\"CHL\"].values\n",
    "log_chl = np.log10(chl)\n",
    "# robust range (avoid outliers)\n",
    "vmin, vmax = np.nanpercentile(log_chl, [5, 95])\n",
    "norm = np.clip((log_chl - vmin) / (vmax - vmin), 0, 1)\n",
    "cmap = plt.get_cmap(\"viridis\")\n",
    "rgba = cmap(norm)  # values in [0,1]\n",
    "\n",
    "df[\"r\"] = (rgba[:, 0] * 255).astype(int)\n",
    "df[\"g\"] = (rgba[:, 1] * 255).astype(int)\n",
    "df[\"b\"] = (rgba[:, 2] * 255).astype(int)\n",
    "\n",
    "layer = pdk.Layer(\n",
    "    \"ScatterplotLayer\",\n",
    "    data=df,\n",
    "    get_position=[\"longitude\", \"latitude\"],\n",
    "    get_fill_color=\"[r, g, b, 300]\",\n",
    "    get_radius=2000,\n",
    "    pickable=True,\n",
    ")\n",
    "\n",
    "view_state = pdk.ViewState(\n",
    "    latitude=float(df.latitude.mean()),\n",
    "    longitude=float(df.longitude.mean()),\n",
    "    zoom=3,\n",
    ")\n",
    "\n",
    "deck = pdk.Deck(\n",
    "    layers=[layer],\n",
    "    initial_view_state=view_state,\n",
    ")\n",
    "\n",
    "deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dffbad-8030-4ac6-b3c8-4d63908a995b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Integrated NPP Data from CalCOFI Bottle Survey\n",
    "npp = pd.read_csv('../data/CalCOFI_Integrated_NPP.csv', index_col=0)\n",
    "npp = npp[['Date', 'Latitude', 'Longitude', 'Integrated_NPP']]\n",
    "npp['Date'] = pd.to_datetime(npp['Date'])\n",
    "npp = npp.sort_values(by='Date').reset_index(drop=True)\n",
    "npp = npp[(npp.Date >= '1998-01-01') & (npp.Date <= '2007-12-31')].reset_index(drop=True)\n",
    "\n",
    "# Get CHL latitude/longitude grid\n",
    "hdf_file = \"../data/cal_aco_3840_Latitude_Longitude.hdf\"\n",
    "hdf = SD(hdf_file, SDC.READ)\n",
    "lat_name = list(hdf.datasets().keys())[0]\n",
    "lon_name = list(hdf.datasets().keys())[1]\n",
    "lats = hdf.select(lat_name)\n",
    "lons = hdf.select(lon_name)\n",
    "lats = lats[:]\n",
    "lons = lons[:]\n",
    "\n",
    "tree = cKDTree(np.column_stack([lats.ravel(), lons.ravel()]))\n",
    "\n",
    "# Query the KDTree for each NPP row\n",
    "lat_lon = npp[['Latitude', 'Longitude']].values\n",
    "distances, indices = tree.query(lat_lon)\n",
    "\n",
    "npp_sat = npp.copy()\n",
    "npp_sat['chl_sat_idx'] = indices\n",
    "\n",
    "# Assume da has dims (\"time\", \"y\", \"x\") and chl_sat_idx is in npp_sat\n",
    "flat_indices = npp_sat['chl_sat_idx'].values\n",
    "\n",
    "# Convert flat indices to 2D pixel indices (y, x)\n",
    "y_idx, x_idx = np.unravel_index(flat_indices, da.shape[1:])  # skip time dim\n",
    "\n",
    "# Store as a tuple array in a new column\n",
    "npp_sat['chl_sat_grid_idx'] = list(zip(y_idx, x_idx))\n",
    "\n",
    "# Calcualte grid size and min_valid_pixels for satellite data\n",
    "res_km = 9\n",
    "grid_size, min_valid_pixels = calculate_grid_window(res_km)\n",
    "\n",
    "print(min_valid_pixels)\n",
    "\n",
    "half = grid_size // 2\n",
    "\n",
    "# Get the central y/x indices as arrays\n",
    "y_idx = np.array([y for y, x in npp_sat['chl_sat_grid_idx']])\n",
    "x_idx = np.array([x for y, x in npp_sat['chl_sat_grid_idx']])\n",
    "\n",
    "# Compute relative offsets for the grid\n",
    "offsets = np.arange(-half, half + 1)  # [-1, 0, 1]\n",
    "dy, dx = np.meshgrid(offsets, offsets, indexing='ij')  # shape (3,3)\n",
    "\n",
    "# Broadcast to all rows\n",
    "y_grid = y_idx[:, None, None] + dy[None, :, :]  # shape (n_rows, 3, 3)\n",
    "x_grid = x_idx[:, None, None] + dx[None, :, :]  # shape (n_rows, 3, 3)\n",
    "\n",
    "# Clip indices to valid range\n",
    "y_grid = np.clip(y_grid, 0, da.shape[1] - 1)\n",
    "x_grid = np.clip(x_grid, 0, da.shape[2] - 1)\n",
    "\n",
    "# Store as a single array of tuples per row if you want\n",
    "chl_grid_idxs = np.array(list(zip(y_grid.reshape(len(npp_sat), -1),\n",
    "                                  x_grid.reshape(len(npp_sat), -1))))\n",
    "# This gives an array of shape (n_rows, 9, 2), i.e., 3x3 flattened\n",
    "npp_sat['chl_grid_idxs'] = list(chl_grid_idxs)\n",
    "\n",
    "display(npp_sat)\n",
    "\n",
    "display(npp_sat.loc[0,'chl_grid_idxs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc7418-d1ef-43fc-82fc-bde5af89c9c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydeck as pdk\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Extract lat/lon for satellite windows\n",
    "# -----------------------------\n",
    "\n",
    "def flatten_grid_latlon(row, lats, lons):\n",
    "    \"\"\"\n",
    "    Given a row with chl_grid_idxs (3x3 or grid_size x grid_size), \n",
    "    return a DataFrame with columns latitude, longitude, and npp_id.\n",
    "    \"\"\"\n",
    "    grid_idxs = np.array(row['chl_grid_idxs'])  # shape: 2 x 9 (or 9x2?)\n",
    "    \n",
    "    # If your chl_grid_idxs is a list of [y_list, x_list] for 3x3\n",
    "    # convert to numpy array of shape (2, n_pixels)\n",
    "    if len(grid_idxs) == 2 and len(grid_idxs[0]) == len(grid_idxs[1]):\n",
    "        y_idx, x_idx = grid_idxs\n",
    "    else:\n",
    "        # handle case where it's list of 2D pairs [[y1,x1],...]\n",
    "        grid_array = np.array(grid_idxs)\n",
    "        y_idx = grid_array[:,0]\n",
    "        x_idx = grid_array[:,1]\n",
    "    \n",
    "    # extract lat/lon from the original arrays\n",
    "    lat_vals = lats[y_idx, x_idx]\n",
    "    lon_vals = lons[y_idx, x_idx]\n",
    "    \n",
    "    # return a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'latitude': lat_vals,\n",
    "        'longitude': lon_vals,\n",
    "        'npp_idx': row.name  # link to original NPP point\n",
    "    })\n",
    "    return df\n",
    "\n",
    "# Flatten all rows into one DataFrame for satellite points\n",
    "sat_points_list = [flatten_grid_latlon(row, lats, lons) for _, row in npp_sat.iterrows()]\n",
    "sat_points_df = pd.concat(sat_points_list, ignore_index=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Prepare NPP points\n",
    "# -----------------------------\n",
    "npp_points_df = npp_sat[['Latitude', 'Longitude']].copy()\n",
    "npp_points_df.rename(columns={'Latitude':'latitude', 'Longitude':'longitude'}, inplace=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Create PyDeck layers\n",
    "# -----------------------------\n",
    "# NPP points layer (red)\n",
    "npp_layer = pdk.Layer(\n",
    "    \"ScatterplotLayer\",\n",
    "    data=npp_points_df,\n",
    "    get_position='[longitude, latitude]',\n",
    "    get_color='[255, 0, 0]',\n",
    "    get_radius=200,\n",
    "    pickable=True,\n",
    "    auto_highlight=True,\n",
    "    tooltip=True,\n",
    ")\n",
    "\n",
    "# Satellite grid points layer (blue)\n",
    "sat_layer = pdk.Layer(\n",
    "    \"ScatterplotLayer\",\n",
    "    data=sat_points_df,\n",
    "    get_position='[longitude, latitude]',\n",
    "    get_color='[0, 0, 255]',\n",
    "    get_radius=200,\n",
    "    pickable=True,\n",
    "    auto_highlight=True,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Create Deck\n",
    "# -----------------------------\n",
    "mid_lat = npp_points_df['latitude'].mean()\n",
    "mid_lon = npp_points_df['longitude'].mean()\n",
    "\n",
    "r = pdk.Deck(\n",
    "    layers=[npp_layer, sat_layer],\n",
    "    initial_view_state=pdk.ViewState(\n",
    "        latitude=mid_lat,\n",
    "        longitude=mid_lon,\n",
    "        zoom=5,\n",
    "        pitch=0,\n",
    "    ),\n",
    "    tooltip={\"text\": \"Latitude: {latitude}\\nLongitude: {longitude}\"}\n",
    ")\n",
    "\n",
    "# Display in Jupyter\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433b5d9-4554-45bd-a08a-1249050b1de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chl_window(i, npp_sat, max_day_offset=3, min_valid_pixels=3):\n",
    "    row = npp_sat.iloc[i]\n",
    "    date = row['Date']\n",
    "    grid_idxs = np.array(row['chl_grid_idxs'])\n",
    "    orig_date = pd.to_datetime(date)\n",
    "\n",
    "    for offset in range(max_day_offset + 1):\n",
    "        for direction in [-1, 1] if offset > 0 else [0]:\n",
    "            current_offset = lag + (direction + offset)\n",
    "            target_date = orig_date + pd.Timedelta(days=current_offset)\n",
    "            date_str = target_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "            if date.strftime('%Y-%m-%d') in chl_index:\n",
    "                chl_file = chl_index[target_date.strftime('%Y-%m-%d')]\n",
    "                chl_data = load_chl_day(chl_file)\n",
    "            \n",
    "                # Make xarray\n",
    "                da = xr.DataArray(\n",
    "                    data=chl_data[np.newaxis, :, :],  # add time dim\n",
    "                    dims=(\"time\", \"y\", \"x\"),\n",
    "                    coords={\n",
    "                        \"time\": [np.datetime64(target_date)],\n",
    "                        \"latitude\": ((\"y\",\"x\"), lats),\n",
    "                        \"longitude\": ((\"y\",\"x\"), lons),\n",
    "                    },\n",
    "                    name=\"CHL\",\n",
    "                )\n",
    "                \n",
    "                # Extract values for 3x3 grid\n",
    "                time_idx = 0  # first/only time\n",
    "                y_idx = grid_idxs[0, :]\n",
    "                x_idx = grid_idxs[1, :]\n",
    "            \n",
    "                chl_window = da.values[time_idx, y_idx, x_idx].reshape(grid_size, grid_size)\n",
    "                \n",
    "                # Check for validity\n",
    "                valid_count = np.isfinite(chl_window).sum()\n",
    "                if valid_count >= min_valid_pixels:\n",
    "                    return i, chl_window, current_offset\n",
    "            \n",
    "    else:\n",
    "        return i, None, None\n",
    "\n",
    "def run_get_chl_window(i, npp_sat, max_day_offset, min_valid_pixels):\n",
    "    try:\n",
    "        i, window, offset = get_chl_window(i, npp_sat, max_day_offset, min_valid_pixels)\n",
    "        return i, window, offset\n",
    "    except:\n",
    "        return i, None, None\n",
    "\n",
    "\n",
    "max_day_offset = 12\n",
    "lag = 0\n",
    "\n",
    "var = 'CHL'\n",
    "\n",
    "col_values = f'{var}_values_lag_{lag}'\n",
    "col_offset = f'{var}_offset_days_lag_{lag}'\n",
    "npp_sat[col_values] = None\n",
    "npp_sat[col_offset] = None\n",
    "\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(run_get_chl_window)(\n",
    "        i, npp_sat, max_day_offset, min_valid_pixels\n",
    "    ) for i in range(len(npp))\n",
    ")\n",
    "\n",
    "# Add results to NPP dataframe\n",
    "for result in results:\n",
    "    if result is not None:\n",
    "        i, window, offset = result\n",
    "        \n",
    "        npp_sat.at[i, col_values] = window\n",
    "        npp_sat.at[i, col_offset] = offset\n",
    "\n",
    "\n",
    "# for i in range(len(npp)):\n",
    "\n",
    "#     out = get_chl_window(i, date, grid_idxs, max_day_offset=3, min_valid_pixels=3)\n",
    "#     display(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1624b90-797d-4cba-a902-f151ea8bdfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_eppley_model(row):\n",
    "    \"\"\"\n",
    "    Computes NPP using the Eppley square root model for nested array data.\n",
    "    \"\"\"\n",
    "    chl_data = row.get('CHL_values_lag_0')\n",
    "    \n",
    "    # Handle None, NaNs, or empty entries\n",
    "    if chl_data is None or (isinstance(chl_data, float) and np.isnan(chl_data)):\n",
    "        return pd.Series({'Satellite_NPP (ESQRT)': np.nan, 'Satellite_NPP_std (ESQRT)': np.nan})\n",
    "\n",
    "    # Convert to numpy array and flatten in case it's a list of lists\n",
    "    chl_array = np.array(chl_data).flatten()\n",
    "    \n",
    "    # Filter for valid, positive chlorophyll values\n",
    "    valid_chl = chl_array[(chl_array > 0) & np.isfinite(chl_array)]\n",
    "\n",
    "    if len(valid_chl) == 0:\n",
    "        return pd.Series({'Satellite_NPP (ESQRT)': np.nan, 'Satellite_NPP_std (ESQRT)': np.nan})\n",
    "\n",
    "    # The Eppley Square Root Model: NPP = 1000 * sqrt(Chl)\n",
    "    npp_estimates = 1000 * np.sqrt(valid_chl)\n",
    "\n",
    "    return pd.Series({\n",
    "        'Satellite_NPP (ESQRT)': np.mean(npp_estimates),\n",
    "        'Satellite_NPP_std (ESQRT)': np.std(npp_estimates)\n",
    "    })\n",
    "\n",
    "# Apply the function across the rows\n",
    "eppley_results = npp_sat.apply(apply_eppley_model, axis=1)\n",
    "npp_eppley = pd.concat([npp, eppley_results], axis=1)\n",
    "npp_eppley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20e47f-d510-403d-b999-f48ed914f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out zero values for log transform\n",
    "\n",
    "result_df = npp_eppley.copy()\n",
    "result_df = result_df[result_df['Integrated_NPP'] > 0]\n",
    "\n",
    "# Create plot\n",
    "plt_ = make_scatter_plot(result_df, 'ESQRT')\n",
    "plt_.ylim(10**1.75,10**4)\n",
    "plt_.xlim(10**1.75,10**4)\n",
    "plt_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78296732-64c3-4c2f-8aee-2c5680ee7567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sat_values(i, npp, chl_index, grid_size, max_day_offset=3, min_valid_pixels=3):\n",
    "#     row_npp = npp.loc[i]\n",
    "#     target_lat, target_lon = row_npp['Latitude'], row_npp['Longitude']\n",
    "#     orig_date = pd.to_datetime(row_npp['Date'])\n",
    "    \n",
    "#     # 1. Load data for the day\n",
    "#     date_str = orig_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "#     # if date_str not in chl_index:\n",
    "#     #     return i, None, None # Or handle offset search here\n",
    "\n",
    "#     chl_data, p_info = load_chl_day(chl_index[date_str])\n",
    "    \n",
    "#     # 2. Get the exact pixel for this Lat/Lon in Albers space\n",
    "#     row_idx, col_idx = latlon_to_pixel(target_lat, target_lon, p_info)\n",
    "    \n",
    "#     # 3. Extract n x n window. Search day offsets (0, then ±1, ±2, up to max_day_offset)\n",
    "#     half = grid_size // 2\n",
    "#     for offset in range(max_day_offset + 1):\n",
    "#         for direction in [-1, 1] if offset > 0 else [0]:\n",
    "#             current_offset = lag + (direction * offset)\n",
    "#             target_date = orig_date + pd.Timedelta(days=current_offset)\n",
    "#             date_str = target_date.strftime('%Y-%m-%d')\n",
    "\n",
    "#             # Load the full day's data\n",
    "#             if date_str not in chl_index:\n",
    "#                 return i, None, None\n",
    "                \n",
    "#             chl_data, p_info = load_chl_day(chl_index[date_str])\n",
    "            \n",
    "\n",
    "#             # Get window\n",
    "#             window = chl_data[row_idx-half : row_idx+half+1, \n",
    "#                               col_idx-half : col_idx+half+1]\n",
    "                \n",
    "#             # Check for validity\n",
    "#             valid_count = np.isfinite(window).sum()\n",
    "#             if valid_count >= min_valid_pixels:\n",
    "#                 return i, window, current_offset\n",
    "\n",
    "#     return i, None, None\n",
    "\n",
    "# def run_get_sat_values(i, npp, chl_index, grid_size, max_day_offset=3, min_valid_pixels=3):\n",
    "#     try:\n",
    "#         i, window, offset = get_sat_values(i, npp, chl_index, grid_size, max_day_offset, min_valid_pixels)\n",
    "#         return i, window, offset\n",
    "#     except:\n",
    "#         return i, None, None\n",
    "\n",
    "# Load Integrated NPP Data from CalCOFI Bottle Survey\n",
    "npp = pd.read_csv('../data/CalCOFI_Integrated_NPP.csv', index_col=0)\n",
    "npp = npp[['Date', 'Latitude', 'Longitude', 'Integrated_NPP']]\n",
    "npp['Date'] = pd.to_datetime(npp['Date'])\n",
    "npp = npp.sort_values(by='Date').reset_index(drop=True)\n",
    "npp = npp[(npp.Date >= '1998-01-01') & (npp.Date <= '2007-12-31')].reset_index(drop=True)\n",
    "\n",
    "# Calcualte grid size and min_valid_pixels for satellite data\n",
    "res_km = 9\n",
    "grid_size, min_valid_pixels = calculate_grid_window(res_km)\n",
    "\n",
    "# Add satellite data to the NPP dataframe\n",
    "input_vars = ['CHL'] #list(satellite_ds.data_vars)\n",
    "\n",
    "lags = [0] #[-28, -21, -14, -7, 0, 7, 14, 21, 28]\n",
    "particle_tracking = False\n",
    "\n",
    "total_iterations = len(input_vars) * len(lags)\n",
    "# with tqdm(total=total_iterations) as pbar:\n",
    "for var in input_vars:\n",
    "    for lag in lags:\n",
    "        print(var, lag)\n",
    "        npp = npp[['Date', 'Latitude', 'Longitude', 'Integrated_NPP']].copy()\n",
    "        col_values = f'{var}_values_lag_{lag}'\n",
    "        col_offset = f'{var}_offset_days_lag_{lag}'\n",
    "        npp[col_values] = None\n",
    "        npp[col_offset] = None\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(\n",
    "            delayed(run_get_sat_values)(\n",
    "                i, npp, chl_index, grid_size\n",
    "            ) for i in range(len(npp))\n",
    "        )\n",
    "        \n",
    "        # Add results to NPP dataframe\n",
    "        for result in results:\n",
    "            if result is not None:\n",
    "                i, window, offset = result\n",
    "                \n",
    "                npp.at[i, col_values] = window\n",
    "                npp.at[i, col_offset] = offset\n",
    "\n",
    "            # # Save individual columns immediately after computation\n",
    "            # if particle_tracking == True:\n",
    "            #     save_folder_name = f'npp_columns/{var}_lag_{lag}.pkl'\n",
    "            # else:\n",
    "            #     save_folder_name = f'npp_columns_no_particle_tracking/{var}_lag_{lag}.pkl'\n",
    "            # with open(save_folder_name, 'wb') as f:\n",
    "            #     pickle.dump({\n",
    "            #         col_values: npp[col_values],\n",
    "            #         col_offset: npp[col_offset]\n",
    "            #     }, f)\n",
    "                \n",
    "            # pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ffd181-e963-4bd5-8fc6-a7911d53fc87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "npp_sat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8d8b7-3ef6-4d83-8480-aaccee2abf0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "npp_eppley = pd.concat([npp, eppley_results], axis=1)\n",
    "npp_eppley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c22490-bf6b-4d07-a844-ac36ac068cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out zero values for log transform\n",
    "\n",
    "result_df = npp_sat.copy()\n",
    "result_df = result_df[['Integrated_NPP', 'Satellite_NPP (ESQRT)', 'Satellite_NPP_std (ESQRT)']]\n",
    "result_df = result_df[result_df['Integrated_NPP'] > 0]\n",
    "\n",
    "# Create plot\n",
    "plt = make_scatter_plot(result_df, 'ESQRT')\n",
    "plt.ylim(10**1.75,10**4)\n",
    "plt.xlim(10**1.75,10**4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c493ae-88fa-4fc1-b0e8-d0c5675f3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_scatter_plot(result_df, model_name, ax=None):\n",
    "\n",
    "    # # Calculate summary statistics (on log-transformed data)\n",
    "    result_df = result_df[['Integrated_NPP', f'Satellite_NPP ({model_name})', f'Satellite_NPP_std ({model_name})']].copy().dropna()\n",
    "    log_x = np.log10(result_df['Integrated_NPP'].values)\n",
    "    log_y = np.log10(result_df[f'Satellite_NPP ({model_name})'].values)\n",
    "\n",
    "    # Linear regression in log-log space\n",
    "    model = LinearRegression()\n",
    "    model.fit(log_x.reshape(-1, 1), log_y)\n",
    "    log_y_pred = model.predict(log_x.reshape(-1, 1))\n",
    "    r2 = r2_score(log_y, log_y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(log_y, log_y_pred))\n",
    "    rmsd = np.sqrt(np.mean((log_y - log_y_pred)**2))\n",
    "    slope = model.coef_[0]\n",
    "    intercept = model.intercept_\n",
    "    corr = np.corrcoef(result_df['Integrated_NPP'], result_df[f'Satellite_NPP ({model_name})'])[0,1]\n",
    "    log_corr = np.corrcoef(log_x, log_y)[0,1]\n",
    "    n = len(result_df)\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(6,6))\n",
    "    \n",
    "    ax.errorbar(\n",
    "        result_df['Integrated_NPP'], \n",
    "        result_df[f'Satellite_NPP ({model_name})'], \n",
    "        yerr=result_df[f'Satellite_NPP_std ({model_name})'], \n",
    "        fmt='none',           # no marker (we already have scatterplot)\n",
    "        ecolor='gray',        # color of error bars\n",
    "        alpha=0.4,            # transparency\n",
    "        capsize=2,            # caps on error bars\n",
    "        elinewidth=1,\n",
    "        label='±1 Std Dev',\n",
    "        zorder=1\n",
    "    )\n",
    "    \n",
    "    # Re-transform regression line to original scale\n",
    "    x_range = np.logspace(np.log10(result_df['Integrated_NPP'].min()), np.log10(result_df['Integrated_NPP'].max()), 100)\n",
    "    y_fit = 10 ** model.predict(np.log10(x_range).reshape(-1, 1))\n",
    "    \n",
    "    if ax is None:\n",
    "        sns.scatterplot(data=result_df, x='Integrated_NPP', y=f'Satellite_NPP ({model_name})', s=20, alpha=0.8)\n",
    "    else:\n",
    "        sns.scatterplot(data=result_df, x='Integrated_NPP', y=f'Satellite_NPP ({model_name})', ax=ax, s=20, alpha=0.8)\n",
    "        \n",
    "    # Add regression line\n",
    "    ax.plot(x_range, y_fit, color='red', label='Regression line')\n",
    "    \n",
    "    # Add 1:1, 1:2, and 1:3 lines\n",
    "    min_val = min(result_df[['Integrated_NPP', f'Satellite_NPP ({model_name})']].min())\n",
    "    max_val = max(result_df[['Integrated_NPP', f'Satellite_NPP ({model_name})']].max())\n",
    "    ax.plot(x_range, x_range, 'k--', label='1:1 line', alpha=0.5)\n",
    "    ax.plot(x_range, x_range / 2, 'k:', label='1:2 line', alpha=0.5)\n",
    "    ax.plot(x_range, x_range / 3, 'k-.', label='1:3 line', alpha=0.5)\n",
    "    \n",
    "    # Labels, scales, title\n",
    "    ax.set_xlabel('In situ NPP (mg C/m²/day)')\n",
    "    ax.set_ylabel(f'Satellite NPP ({model_name})')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.set_title(f'Satellite vs. In Situ NPP ({model_name})')\n",
    "\n",
    "    # Add R², RMSE, N as text\n",
    "    ax.text(0.05, 0.95, \n",
    "            f'$R^2$ = {r2:.2f}\\n'\n",
    "            f'RMSE = {rmse:.2f}\\n'\n",
    "            f'RMSD = {rmsd:.2f}\\n'\n",
    "            f'Slope = {slope:.3f}\\nIntercept = {intercept:.3f}\\n'\n",
    "            f'ρ = {corr:.3f}\\n'\n",
    "            f'ρ (log-log) = {log_corr:.3f}\\n'\n",
    "            f'N = {n}',\n",
    "            transform=ax.transAxes, \n",
    "            verticalalignment='top',\n",
    "            fontsize=9,\n",
    "            bbox=dict(boxstyle=\"round\", facecolor='white', alpha=0.8))\n",
    "    \n",
    "    ax.grid(True, which='both', ls=':')\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbad3a-9420-4c6b-bf05-c302610b3c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_day_length(row):\n",
    "    try:\n",
    "        date = pd.to_datetime(row['Date']).date()\n",
    "        observer = Observer(latitude=row['Latitude'], longitude=row['Longitude'])\n",
    "        s = sun(observer, date=date)\n",
    "\n",
    "        # Convert UTC times to local timezone (e.g., US/Pacific)\n",
    "        local_tz = pytz.timezone('US/Pacific')\n",
    "\n",
    "        sunrise_local = s['sunrise'].astimezone(local_tz)\n",
    "        sunset_local = s['sunset'].astimezone(local_tz)\n",
    "\n",
    "        if sunset_local < sunrise_local:\n",
    "            sunset_local += timedelta(days=1)\n",
    "\n",
    "        # Calculate day length in hours\n",
    "        day_length_hours = (sunset_local - sunrise_local).total_seconds() / 3600\n",
    "\n",
    "        return day_length_hours\n",
    "        \n",
    "    except Exception as e:\n",
    "        return None\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
